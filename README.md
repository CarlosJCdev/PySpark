# ğŸ§  PySpark Data Engineering

Repositorio con scripts y notebooks utilizados para la gestiÃ³n de datos utilizando PySpark y SQL en entornos distribuidos. Este repositorio incluye buenas prÃ¡cticas, snippets reutilizables, y soluciones a problemas comunes al manejar grandes volÃºmenes de datos.

## ğŸš€ Funcionalidades y tareas comunes incluidas

### ğŸ”„ Ingesta y transformaciÃ³n de datos
- ğŸ“¥ Lectura de datos en formatos como Parquet, CSV, Delta, JSON desde ADLS o sistemas externos.
- ğŸ”— UniÃ³n y limpieza de datos provenientes de SAP
- ğŸ§½ NormalizaciÃ³n y validaciÃ³n de estructuras multi-tabla antes de persistencia.
- ğŸ•¹ï¸ AutomatizaciÃ³n de procesos de ingestiÃ³n por fecha, con widgets o parÃ¡metros externos

### ğŸ§® Procesamiento con PySpark
- ğŸ“Š CÃ¡lculo de agregaciones (por dÃ­a, mes, paÃ­s, etc.) usando `groupBy` y `agg`.
- ğŸ” ComparaciÃ³n entre datasets (por ejemplo, diferencias UDL vs MDL).
- ğŸ§¬ CÃ¡lculo de mÃ©tricas de calidad de datos (nulos, duplicados, desviaciones).
- ğŸ§  Uso de funciones personalizadas (`udf`, `withColumn`, `lit`, `when`).
- ğŸ§° AplicaciÃ³n de filtros dinÃ¡micos y joins complejos con condiciones mÃºltiples.

### ğŸ§¾ SQL sobre Spark
- ğŸ“ Consultas SQL sobre DataFrames o tablas registradas en el metastore (`%sql` en Databricks).
- âš–ï¸ ComparaciÃ³n de datos con `EXCEPT`, `UNION ALL`, `JOIN` y `CASE WHEN`.
- ğŸš€ OptimizaciÃ³n de queries con vistas temporales y persistencia intermedia.

### ğŸ§° GestiÃ³n de formatos y particiones
- ğŸ—‚ï¸ Escritura de datos en tablas Delta particionadas (`partitionBy("COUNTRY", "FILE_DATE")`).
- ğŸ” MigraciÃ³n de tablas Parquet a Delta Lake para mejorar consistencia.
- ğŸ§¾ Lectura de `_delta_log` y control de versiones.

### ğŸ” Procesos programados y monitoreo
- âœ… GeneraciÃ³n de validaciones diarias y mensuales de integridad entre capas.
- âš ï¸ DetecciÃ³n automÃ¡tica de desviaciones de mÃ¡s del 5% en mÃ©tricas clave.
- â±ï¸ Control de ejecuciones por timestamp (`EXECUTION_TIMESTAMP`).
- ğŸ“§ EnvÃ­o de alertas por correo vÃ­a Databricks Workflow o Azure Data Factory.

## ğŸ“¦ CÃ³digos y patrones populares incluidos

- ğŸ”— `unionByName` para combinar mÃºltiples DataFrames.
- ğŸ§© `merge` tipo `MERGE INTO` para actualizaciones eficientes de tablas Delta.
- â“ `isEmpty()` y `count()` como validaciÃ³n previa a escritura.
- ğŸ§± Uso de funciones `create_schema`, `truncate_table`, `get_latest_hour`.
- âš™ï¸ ModularizaciÃ³n del cÃ³digo con funciones como:
  - `ret_mdl(granularity)`
  - `ret_udl(granularity)`
  - `reading_source_date(start, end)`
  - `loading_temp(df, path, table_type)`
